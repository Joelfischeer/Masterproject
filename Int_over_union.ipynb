{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 11:18:21,826:WARNING:From c:\\Users\\Joel Fischer\\.conda\\envs\\segmentationenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "# Numpy for numpy.arrays\n",
    "import numpy as np\n",
    "\n",
    "# Include ITK for DICOM reading.\n",
    "import itk\n",
    "\n",
    "# Include pydicom_seg for DICOM SEG objects\n",
    "import pydicom\n",
    "import pydicom_seg\n",
    "\n",
    "# for downloading data from TCIA\n",
    "from tcia_utils import nbia\n",
    "\n",
    "# This is the most common import command for itkWidgets.\n",
    "#   The view() function opens an interactive viewer for 2D and 3D\n",
    "#   data in a variety of formats.\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the pirads file and change the rownames:\n",
    "pirads = pd.read_excel('Pirads_updated_age_PSA.xlsx')\n",
    "pirads.rename(columns={'Unnamed: 0': 'lesion_name'},inplace=True)\n",
    "pirads.index = list(pirads['lesion_name'])\n",
    "pirads.drop('lesion_name',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = pd.read_excel('C:\\\\Users\\\\Joel Fischer\\\\Documents\\\\Masterarbeit\\\\Studie\\\\Projekt\\\\Auswertungsdaten\\\\Benchmark\\\\translation.xlsx')\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "translation.drop(8,inplace=True) #remove the bell_0027 case as its metadata is missing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all the lesion segmentations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all lesion filenames:\n",
    "lesion_filenames = []\n",
    "directory = 'E:\\\\ksa_study_data'\n",
    "for path, dirs, filenames in os.walk(directory):\n",
    "    for f in filenames:\n",
    "        if f.endswith('.nii.gz'):\n",
    "            #get only the files of the 67 Cases we have biopsied totally:\n",
    "            name = f.split('_')[0].split('-')[0] + '_' + f.split('_')[0].split('-')[1]\n",
    "            #we need to manual add the bell_0045 Case as it is missing in the translation:\n",
    "            if name in list(translation['Case']) or name == 'bell_0045':\n",
    "                #we only want lesions of t2w images, not full prostate segmentations or t2w images:\n",
    "                if f.split('.')[0].split('_')[-1] != 'none':\n",
    "                    if f.split('_')[1] == 't2w':\n",
    "                        if f.split('_')[2] != 'pro':\n",
    "                            lesion_filenames.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the IoU,DSC,Fscore for pooled segmentations for each reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def IoU_of_two_segmentations(segA,segB):\n",
    "    # segmentation_A: lesion mask of a lesion\n",
    "    # segmentation_A: lesion mask of a different lesion\n",
    "    intersection = np.sum(segA*segB)\n",
    "    fsum = np.sum(segA)\n",
    "    ssum = np.sum(segB)\n",
    "    union = fsum + ssum - intersection\n",
    "    iou = (intersection) / (union)\n",
    "    iou = round(iou, 3)\n",
    "    return iou\n",
    "\n",
    "def DICE_COE(segmentation_A, segmentation_B):\n",
    "    # segmentation_A: lesion mask of a lesion\n",
    "    # segmentation_A: lesion mask of a different lesion\n",
    "    intersection = np.sum(segmentation_A*segmentation_B)\n",
    "    fsum = np.sum(segmentation_A)\n",
    "    ssum = np.sum(segmentation_B)\n",
    "    dice = (2 * intersection) / (fsum + ssum)\n",
    "    dice = round(dice, 3)\n",
    "    return (dice)\n",
    "\n",
    "def fbeta_score(segA,segB,beta):\n",
    "    '''We want to limit the impact of the oversensitivity of the FUSE-AI solution by penalizing its False Negatives more. \n",
    "    This way the super large volumne is ignored '''\n",
    "    intersection = np.sum(segA*segB)\n",
    "    fsum = np.sum(segA)\n",
    "    ssum = np.sum(segB)\n",
    "    fn = ssum - intersection\n",
    "    fp = fsum - intersection\n",
    "    fscore = ((1+beta**2)*intersection)/(((1+beta**2)*intersection)+(beta**2 * fn) + fp)\n",
    "    return round(fscore,3)\n",
    "\n",
    "def get_segmentation_array(filename):\n",
    "    seg = nib.load(filename)\n",
    "    return seg.get_fdata()\n",
    "\n",
    "def get_list_of_segmentations(filename_list):\n",
    "    segmentation_list = []\n",
    "    for filename in range(len(filename_list)):\n",
    "        Case = filename_list[filename].split('-')[0]\n",
    "        Case_name = filename_list[filename].split('_')[0]\n",
    "        seg = get_segmentation_array(f\"E:/ksa_study_data/{Case}/{Case_name}/{filename_list[filename]}\")\n",
    "        segmentation_list.append(seg)\n",
    "    return segmentation_list\n",
    "\n",
    "def get_filenames_of_Case(Case_segmentations):\n",
    "    n_pcai100_auto = []\n",
    "    n_rad1_rad = []\n",
    "    n_rad1_ass = []\n",
    "    n_rad2_rad = []\n",
    "    n_rad2_ass = []\n",
    "    n_rad3_rad = []\n",
    "    n_rad3_ass = []\n",
    "\n",
    "    for segmentation in Case_segmentations:\n",
    "        if segmentation.split('.')[-3].split('_')[-1] == 'auto' and segmentation.split('.')[-3].split('_')[-2] == 'pcai100':\n",
    "            n_pcai100_auto.append(segmentation)\n",
    "        if segmentation.split('.')[-3].split('_')[-1] == 'rad' and segmentation.split('.')[-3].split('_')[-2] == 'rad1':\n",
    "            n_rad1_rad.append(segmentation)\n",
    "        if segmentation.split('.')[-3].split('_')[-1] == 'ass' and segmentation.split('.')[-3].split('_')[-2] == 'rad1':\n",
    "            n_rad1_ass.append(segmentation)\n",
    "        if segmentation.split('.')[-3].split('_')[-1] == 'rad' and segmentation.split('.')[-3].split('_')[-2] == 'rad2':\n",
    "            n_rad2_rad.append(segmentation)\n",
    "        if segmentation.split('.')[-3].split('_')[-1] == 'ass' and segmentation.split('.')[-3].split('_')[-2] == 'rad2':\n",
    "            n_rad2_ass.append(segmentation)\n",
    "        if segmentation.split('.')[-3].split('_')[-1] == 'rad' and segmentation.split('.')[-3].split('_')[-2] == 'rad3':\n",
    "            n_rad3_rad.append(segmentation)\n",
    "        if segmentation.split('.')[-3].split('_')[-1] == 'ass' and segmentation.split('.')[-3].split('_')[-2] == 'rad3':\n",
    "            n_rad3_ass.append(segmentation)\n",
    "    return(n_pcai100_auto,n_rad1_rad,n_rad1_ass,n_rad2_rad,n_rad2_ass,n_rad3_rad,n_rad3_ass)\n",
    "\n",
    "def make_binary_array(seg_array):\n",
    "    for i in range(seg_array.shape[0]):\n",
    "        for j in range(seg_array.shape[1]):\n",
    "            for k in range(seg_array.shape[2]):\n",
    "                #if a voxel only belongs to array1:\n",
    "                if seg_array[i,j,k] > 1:\n",
    "                    seg_array[i,j,k] = 1\n",
    "    return seg_array\n",
    "\n",
    "\n",
    "def summed_segmentations(segmentation_list):\n",
    "\n",
    "    while len(segmentation_list) > 1: #only add them up, if there are at least 2 segmentations left:\n",
    "\n",
    "        summed_seg = np.where(segmentation_list[0] != 1, 0, 1) + np.where(segmentation_list[1] != 0, 1, 0)\n",
    "        summed_seg_binary = make_binary_array(summed_seg)\n",
    "        del segmentation_list[1]\n",
    "        segmentation_list[0] = summed_seg_binary\n",
    "    return segmentation_list[0]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here a comparison bewteen FUSE-AI prediction and the Reader 1-3 prediction is made. Both, the FUSE-AI and the Reader lesions segmentations are\n",
    " pooled togheter, so that we can then look at the DSC/IoU of the AI vs Reader1_man and AI vs Reader1_ass and see if there is a difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [1:14:59<00:00, 67.16s/it] \n"
     ]
    }
   ],
   "source": [
    "#For each Case, Make an individual Dataframe to store the IoU values:\n",
    "\n",
    "#Get all the Cases:\n",
    "Cases = list(translation['Case'])\n",
    "Cases.append('bell_0045')\n",
    "\n",
    "#For each of the 67 Casees, create a dataframe in a list:\n",
    "IoU_Case_dataframe_list = {Case + '_IoU': pd.DataFrame() for Case in Cases}\n",
    "DSC_Case_dataframe_list = {Case + '_IoU': pd.DataFrame() for Case in Cases}\n",
    "betascore_Case_dataframe_list = {Case + '_IoU': pd.DataFrame() for Case in Cases}\n",
    "\n",
    "\n",
    "for Case in tqdm(Cases):\n",
    "    #Get the Case name with the '-':\n",
    "    Case_name = Case.split('_')[0] + '-' + Case.split('_')[1]\n",
    "    #Get only the filenames for the specific Case:\n",
    "    Case_segmentations = [i for i in lesion_filenames if i.startswith(Case_name)]\n",
    "    #For these segmentations create a into the dataframe list with the dimensions of NxN, N beeing the number of segmentations of this Case:\n",
    "    dataframe_name = Case + '_' + 'IoU'\n",
    "    #create the row and column names:\n",
    "    columns = [Case +'_fuseai',Case +'_rad1_rad',Case +'_rad1_ass',Case +'_rad2_rad',Case +'_rad2_ass',Case +'_rad3_rad',Case +'_rad3_ass']\n",
    "    #Create the dataframe table:\n",
    "    IoU_Case_dataframe_list[dataframe_name] = pd.DataFrame(columns=columns, index=columns)\n",
    "    DSC_Case_dataframe_list[dataframe_name] = pd.DataFrame(columns=columns, index=columns)\n",
    "    betascore_Case_dataframe_list[dataframe_name] = pd.DataFrame(columns=columns, index=columns)\n",
    "    #If there is more than one predicted lesion for the FUSE-AI, we pool the prediction. \n",
    "    filenames_of_segs_to_get_pooled = get_filenames_of_Case(Case_segmentations)\n",
    "    \n",
    "    #For each Entry of this dataframe, calculate the IoU:\n",
    "    for i, rowname in enumerate(IoU_Case_dataframe_list[dataframe_name].index):\n",
    "        #Get the combined segmentation for this Reader (row):\n",
    "        try:\n",
    "            row_segmentation = summed_segmentations(get_list_of_segmentations(filenames_of_segs_to_get_pooled[i]))\n",
    "            \n",
    "        #if the lesion does not exist: make the entire row to 'no comparison':\n",
    "        except:\n",
    "            IoU_Case_dataframe_list[dataframe_name].loc[rowname] = 'no_comparison'\n",
    "            DSC_Case_dataframe_list[dataframe_name].loc[rowname] = 'no_comparison'\n",
    "            betascore_Case_dataframe_list[dataframe_name].loc[rowname] = 'no_comparison'\n",
    "            continue\n",
    "        for j,colname in enumerate(IoU_Case_dataframe_list[dataframe_name].columns):\n",
    "            #Get the combined segmentation for this Reader (column):\n",
    "            try:\n",
    "                col_segmentation = summed_segmentations(get_list_of_segmentations(filenames_of_segs_to_get_pooled[j]))\n",
    "                IoU_Case_dataframe_list[dataframe_name].loc[rowname,colname] = IoU_of_two_segmentations(row_segmentation,col_segmentation)\n",
    "                DSC_Case_dataframe_list[dataframe_name].loc[rowname,colname] = DICE_COE(row_segmentation,col_segmentation)\n",
    "                betascore_Case_dataframe_list[dataframe_name].loc[rowname,colname] = fbeta_score(row_segmentation,col_segmentation,2)\n",
    "\n",
    "\n",
    "            except:\n",
    "                IoU_Case_dataframe_list[dataframe_name].loc[rowname,colname] = 'no_comparison'\n",
    "                DSC_Case_dataframe_list[dataframe_name].loc[rowname,colname] = 'no_comparison'\n",
    "                betascore_Case_dataframe_list[dataframe_name].loc[rowname,colname] = 'no_comparison'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save it as a pickle file:\n",
    "# with open(f\"C:/Users/Joel Fischer/Documents/Masterarbeit/Studie/Projekt/Auswertungsdaten/Benchmark/Lesion_wise_analysis/IoU_combined_segmentations.pickle\",\"wb\") as f:\n",
    "#     pickle.dump(IoU_Case_dataframe_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save it as a pickle file:\n",
    "# with open(f\"C:/Users/Joel Fischer/Documents/Masterarbeit/Studie/Projekt/Auswertungsdaten/Benchmark/Lesion_wise_analysis/DSC_combined_segmentations.pickle\",\"wb\") as f:\n",
    "#     pickle.dump(DSC_Case_dataframe_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    " #save it as a pickle file:\n",
    "# with open(f\"C:/Users/Joel Fischer/Documents/Masterarbeit/Studie/Projekt/Auswertungsdaten/Benchmark/Lesion_wise_analysis/betascore_combined_segmentations.pickle\",\"wb\") as f:\n",
    "#     pickle.dump(betascore_Case_dataframe_list,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part which calculates the iou for every lesion individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each Case, Make an individual Dataframe to store the IoU values:\n",
    "\n",
    "\n",
    "#Get all the Cases:\n",
    "Cases = list(translation['Case'])\n",
    "Cases.append('bell_0045')\n",
    "\n",
    "#For each of the 67 Casees, create a dataframe in a list:\n",
    "IoU_Case_dataframe_list = {Case + '_IoU': pd.DataFrame() for Case in Cases}\n",
    "\n",
    "for Case in ['bell_0001']:\n",
    "    #Get the Case name with the '-':\n",
    "    Case_name = Case.split('_')[0] + '-' + Case.split('_')[1]\n",
    "    #Get only the filenames for the specific Case:\n",
    "    Case_segmentations = [i for i in lesion_filenames if i.startswith(Case_name)]\n",
    "    #For these segmentations create a into the dataframe list with the dimensions of NxN, N beeing the number of segmentations of this Case:\n",
    "    dataframe_name = Case + '_' + 'IoU'\n",
    "    IoU_Case_dataframe_list[dataframe_name] = pd.DataFrame(columns=Case_segmentations, index=Case_segmentations)\n",
    "\n",
    "    #For each Entry of this dataframe, calculate the IoU:\n",
    "    for rowname in IoU_Case_dataframe_list[dataframe_name].index:\n",
    "        for colname in IoU_Case_dataframe_list[dataframe_name].columns:\n",
    "            filename_SegA = f\"E:/ksa_study_data/{Case.split('_')[0]}/{Case_name}/{rowname}\"\n",
    "            filename_SegB = f\"E:/ksa_study_data/{Case.split('_')[0]}/{Case_name}/{colname}\"\n",
    "            IoU_Case_dataframe_list[dataframe_name].loc[rowname,colname] = IoU_of_two_segmentations(filename_SegA,filename_SegB)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmentationenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
